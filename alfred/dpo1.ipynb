{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a14497-b3cb-4868-b928-975d7e5aaaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: noqa\n",
    "# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "# regular:\n",
    "python examples/scripts/dpo.py \\\n",
    "    --dataset_name=trl-internal-testing/hh-rlhf-helpful-base-trl-style \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --learning_rate 1e-3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 500 \\\n",
    "    --output_dir=\"dpo_anthropic_hh\" \\\n",
    "    --warmup_steps 150 \\\n",
    "    --report_to wandb \\\n",
    "    --bf16 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns\n",
    "\n",
    "# peft:\n",
    "python examples/scripts/dpo.py \\\n",
    "    --dataset_name=trl-internal-testing/hh-rlhf-helpful-base-trl-style \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --learning_rate 1e-3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 500 \\\n",
    "    --output_dir=\"dpo_anthropic_hh\" \\\n",
    "    --optim rmsprop \\\n",
    "    --warmup_steps 150 \\\n",
    "    --report_to wandb \\\n",
    "    --bf16 \\\n",
    "    --logging_first_step \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --use_peft \\\n",
    "    --lora_r=16 \\\n",
    "    --lora_alpha=16\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "\n",
    "TRL_USE_RICH = os.environ.get(\"TRL_USE_RICH\", False)\n",
    "\n",
    "from trl.commands.cli_utils import DPOScriptArguments, init_zero_verbose, TrlParser\n",
    "\n",
    "if TRL_USE_RICH:\n",
    "    init_zero_verbose()\n",
    "    FORMAT = \"%(message)s\"\n",
    "\n",
    "    from rich.console import Console\n",
    "    from rich.logging import RichHandler\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from trl import (\n",
    "    DPOConfig,\n",
    "    DPOTrainer,\n",
    "    ModelConfig,\n",
    "    RichProgressCallback,\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    ")\n",
    "\n",
    "\n",
    "if TRL_USE_RICH:\n",
    "    logging.basicConfig(format=FORMAT, datefmt=\"[%X]\", handlers=[RichHandler()], level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fcb49b-192d-49aa-a694-ff1127978f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out TOP_LEVEL\n",
    "printf \"$(git rev-parse --show-toplevel)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1af3c654-96cb-4a0a-ade6-1a5a811bee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = TrlParser((DPOScriptArguments, DPOConfig, ModelConfig))\n",
    "#args, training_args, model_config = parser.parse_args_and_config()\n",
    "\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Force use our print callback\n",
    "if TRL_USE_RICH:\n",
    "    training_args.disable_tqdm = True\n",
    "    console = Console()\n",
    "\n",
    "config = yaml.safe_load(Path(TOP_LEVEL + '/configs/default.yaml').read_text())\n",
    "\n",
    "\n",
    "################\n",
    "# Model & Tokenizer\n",
    "################\n",
    "# torch_dtype = (\n",
    "#     model_config.torch_dtype\n",
    "#     if model_config.torch_dtype in [\"auto\", None]\n",
    "#     else getattr(torch, model_config.torch_dtype)\n",
    "# )\n",
    "config = yaml.safe_load(Path(TOP_LEVEL + '/configs/default.yaml').read_text())\n",
    "match config['model']['torch_dtype']:\n",
    "    case 'float16':\n",
    "        torch_dtype = torch.float16\n",
    "    case 'float32':\n",
    "        torch_dtype = torch.float32\n",
    "    case 'float64':\n",
    "        torch_dtype = torch.float64\n",
    "    case 'bfloat16':\n",
    "        torch_dtype = torch.bfloat16\n",
    "    case 'auto':\n",
    "        torch_dtype = \"auto\"\n",
    "    case _:\n",
    "        raise ValueError('torch_dtype is invalid')\n",
    "    \n",
    "# quantization_config = get_quantization_config(model_config)\n",
    "# model_kwargs = dict(\n",
    "#     revision=model_config.model_revision,\n",
    "#     trust_remote_code=model_config.trust_remote_code,\n",
    "#     attn_implementation=model_config.attn_implementation,\n",
    "#     torch_dtype=torch_dtype,\n",
    "#     use_cache=False if training_args.gradient_checkpointing else True,\n",
    "#     device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "#     quantization_config=quantization_config,\n",
    "# )\n",
    "model = AutoModelForCausalLM.from_pretrained(config['model']['path'], torch_dtype=torch_dtype).to(\"cuda\") #, **model_kwargs)\n",
    "#peft_config = get_peft_config(model_config)\n",
    "#if peft_config is None:\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(config['model']['path']) #, **model_kwargs)\n",
    "# else:\n",
    "#     model_ref = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model']['path'])\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "# if tokenizer.chat_template is None:\n",
    "#     tokenizer.chat_template = \"{% for message in messages %}{{message['role'] + ': ' + message['content'] + '\\n\\n'}}{% endfor %}{{ eos_token }}\"\n",
    "# if args.ignore_bias_buffers:\n",
    "#     # torch distributed hack\n",
    "#     model._ddp_params_and_buffers_to_ignore = [\n",
    "#         name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
    "#     ]\n",
    "\n",
    "# ################\n",
    "# # Optional rich context managers\n",
    "# ###############\n",
    "# init_context = nullcontext() if not TRL_USE_RICH else console.status(\"[bold green]Initializing the DPOTrainer...\")\n",
    "# save_context = (\n",
    "#     nullcontext()\n",
    "#     if not TRL_USE_RICH\n",
    "#     else console.status(f\"[bold green]Training completed! Saving the model to {training_args.output_dir}\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78befc6f-0e60-4040-b94b-fd92f0779804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'chosen', 'rejected'],\n",
      "    num_rows: 7435908\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def return_prompt_and_responses(samples): # -> Dict[str, str, str]:\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            \"Question: \" + question + \"\\n\\nAnswer: \"\n",
    "            for question in samples[\"question\"]\n",
    "        ],\n",
    "        \"chosen\": samples[\"response_j\"],   # rated better than k\n",
    "        \"rejected\": samples[\"response_k\"], # rated worse than j\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"lvwerra/stack-exchange-paired\",\n",
    "    split=\"train\",\n",
    "    data_dir=\"data/rl\"\n",
    ")\n",
    "original_columns = dataset.column_names\n",
    "\n",
    "train_dataset = dataset.map(\n",
    "    return_prompt_and_responses,\n",
    "    batched=True,\n",
    "    remove_columns=original_columns\n",
    ")\n",
    "ds = train_dataset.select(range(100))\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11194a2a-2767-45ad-8573-fb607e3f7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Dataset\n",
    "################\n",
    "# ds = load_dataset(args.dataset_name)\n",
    "# if args.sanity_check:\n",
    "#     for key in ds:\n",
    "#         ds[key] = ds[key].select(range(50))\n",
    "\n",
    "# def process(row):\n",
    "#     row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "#     row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "#     return row\n",
    "\n",
    "# ds = ds.map(\n",
    "#     process,\n",
    "#     num_proc=multiprocessing.cpu_count(),\n",
    "#     load_from_cache_file=False,\n",
    "# )\n",
    "train_dataset = ds #[0:90] #[args.dataset_train_split]\n",
    "#eval_dataset = ds[91:99] #[args.dataset_test_split]\n",
    "#print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae61fd-205f-4217-b3d4-9679727d425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Training\n",
    "################\n",
    "#with init_context:\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "OUTPUT_DIR = TOP_LEVEL + f\"/alfred/output/{config['model']['path']},torch_dtype={torch_dtype}/epoch={NUM_TRAIN_EPOCHS}\"\n",
    "#os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    beta=0.1,\n",
    "    # does not automatically save model output\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    #eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    #peft_config=get_peft_config(model_config),\n",
    "    #callbacks=[RichProgressCallback] if TRL_USE_RICH else None,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#with save_context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "873521b5-138b-4c6f-9d8f-5115f8b35221",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(OUTPUT_DIR+'/final-dpo1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acf559b5-e70d-4ba6-8bca-c87c472abe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(torch_dtype)\n",
    "model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR + '/final-dpo1', torch_dtype=torch_dtype).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR + '/final-dpo1') #config[\"tokenizer\"][\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfe871e8-a31f-4eee-9da4-1f667bebe0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([224619,   1728,    368,  43163,    861,   1306,  15984,    336,  23857,\n",
      "           632,    368,  60614,  29853,     34])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Echo all the statements that are provided.\\nWhy is the sky blue?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ids = torch.tensor(tokenizer(\"Echo all the statements that are provided.\\n\")[\"input_ids\"])\n",
    "query_ids = torch.tensor(tokenizer(\"Why is the sky blue?\")[\"input_ids\"])\n",
    "prompt_and_query_ids = torch.cat([prompt_ids, query_ids], dim=0)\n",
    "print(prompt_and_query_ids)\n",
    "tokenizer.decode(prompt_and_query_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74f2d5bf-ca8a-44d5-aa1e-2599a4a60963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([224619,   1728,    368,  43163,    861,   1306,  15984,    336,  23857,\n",
      "           632,    368,  60614,  29853,     34,   1387,  12300,    427,   1119,\n",
      "          5893,  39152,    664,   2632,  32391,    461,   3595,    267,    567,\n",
      "         47490,      5,    791,  16554,  18681,   1776,  73173,  37287,  10925,\n",
      "            17,   1004,   5827,     15,    718,  64559,  55326,    189,     36,\n",
      "         19182,  12364,    375,    280,    660,   4052,   1002,    654,  12490,\n",
      "            12,    361,   2131,   2782,  24763,   3804,   2592,   9671,     30,\n",
      "           613,   6635,   1380,   2175,  22779,   1256,  13682,   1809,   3784,\n",
      "         32046,  29369,   1331,   3776,  51890,    530,  17393,  27660,    661,\n",
      "          1320,   6648,   1130, 226305,    919,   2914,   3509,   2494,   6168,\n",
      "          6416,   1400,    722,  15397,   3262,   1152,   5382,   6147,  43624,\n",
      "          6054,      2], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Echo all the statements that are provided.\\nWhy is the sky blue? The answer to this question depends on your understanding of what a \"black\" or \"brownish browning\" means. In general, it refers to:\\nA black body (or an object with no light) in which there exists only one color; for example,\\nThe sun\\'s surface has been darkened by its radiation and therefore appears as if it\\'s not shining at any time. (This effect can be seen when you look through windows.)</s>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_ids = model.generate(\n",
    "                  torch.Tensor(prompt_and_query_ids).unsqueeze(0).to(\"cuda\"),\n",
    "    num_beams=1, max_new_tokens=100,              \n",
    "    repetition_penalty=1.2 #,temperature = 0\n",
    "            )\n",
    "print(response_ids[0])\n",
    "tokenizer.batch_decode(response_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279e034-31ba-4aa5-9449-7ca560c8e436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
