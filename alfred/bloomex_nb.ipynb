{
 "cells": [
  {
   "cell_type": "code",
   "id": "7eba4f35-ed22-4de8-8124-49f100a4a593",
   "metadata": {
    "id": "7eba4f35-ed22-4de8-8124-49f100a4a593",
    "ExecuteTime": {
     "end_time": "2024-05-22T13:11:37.294515Z",
     "start_time": "2024-05-22T13:11:37.287240Z"
    }
   },
   "source": [
    "import transformers\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomForTokenClassification\n",
    "from transformers import BloomForTokenClassification\n",
    "from transformers import BloomTokenizerFast\n",
    "import torch\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "5e306dd3-50db-43f1-b7ff-67c84d318535",
   "metadata": {
    "id": "5e306dd3-50db-43f1-b7ff-67c84d318535"
   },
   "source": [
    "# Bloom for Causal Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "id": "d8879880-3c68-450b-adb1-f7a597c2f375",
   "metadata": {
    "id": "d8879880-3c68-450b-adb1-f7a597c2f375",
    "ExecuteTime": {
     "end_time": "2024-05-22T13:11:40.001038Z",
     "start_time": "2024-05-22T13:11:39.162014Z"
    }
   },
   "source": [
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-1b3\", local_files_only=True)\n",
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-1b3\", local_files_only=True)"
   ],
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'bigscience/bloom-1b3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bigscience/bloom-1b3' is the correct path to a directory containing all relevant files for a BloomTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mBloomTokenizerFast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbigscience/bloom-1b3\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m BloomForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbigscience/bloom-1b3\u001B[39m\u001B[38;5;124m\"\u001B[39m, local_files_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/mnt/pycharmprojects/cs234_final/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2094\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   2091\u001B[0m \u001B[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001B[39;00m\n\u001B[1;32m   2092\u001B[0m \u001B[38;5;66;03m# loaded directly from the GGUF file.\u001B[39;00m\n\u001B[1;32m   2093\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(full_file_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m full_file_name \u001B[38;5;129;01min\u001B[39;00m resolved_vocab_files\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m gguf_file:\n\u001B[0;32m-> 2094\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m   2095\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt load tokenizer for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. If you were trying to load it from \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2096\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, make sure you don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt have a local directory with the same name. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2097\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOtherwise, make sure \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is the correct path to a directory \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2098\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontaining all relevant files for a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m tokenizer.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2099\u001B[0m     )\n\u001B[1;32m   2101\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m file_id, file_path \u001B[38;5;129;01min\u001B[39;00m vocab_files\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m   2102\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m file_id \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m resolved_vocab_files:\n",
      "\u001B[0;31mOSError\u001B[0m: Can't load tokenizer for 'bigscience/bloom-1b3'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bigscience/bloom-1b3' is the correct path to a directory containing all relevant files for a BloomTokenizerFast tokenizer."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "60eb2324-38f7-4a8c-afca-683c8fce0cd2",
   "metadata": {
    "id": "60eb2324-38f7-4a8c-afca-683c8fce0cd2",
    "ExecuteTime": {
     "end_time": "2024-05-22T13:11:40.457014Z",
     "start_time": "2024-05-22T13:11:40.418770Z"
    }
   },
   "source": [
    "prompt = \"Ketanji opened the file and stared at its contents, she\"\n",
    "result_length = 50\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKetanji opened the file and stared at its contents, she\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      2\u001B[0m result_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m50\u001B[39m\n\u001B[0;32m----> 3\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m(prompt, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed9f3fef-8dbb-47ec-99a3-61e6a6c947df",
   "metadata": {
    "id": "ed9f3fef-8dbb-47ec-99a3-61e6a6c947df",
    "ExecuteTime": {
     "end_time": "2024-05-22T13:11:41.401033Z",
     "start_time": "2024-05-22T13:11:41.396390Z"
    }
   },
   "outputs": [],
   "source": [
    "#outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "#loss = outputs.loss\n",
    "#logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db68bc2-f556-4353-a40a-28db2f71cdaf",
   "metadata": {
    "id": "2db68bc2-f556-4353-a40a-28db2f71cdaf",
    "outputId": "47170a55-4efb-4904-8c4b-52a75eb1cc0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ketanji opened the file and stared at its contents, she was surprised to see that the contents were not the same as the ones she had seen in the previous book. She was also surprised that she could not remember the names of the characters.\n"
     ]
    }
   ],
   "source": [
    "# Greedy Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                       max_length=result_length,\n",
    "                       no_repeat_ngram_size=2\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "id": "be981c3e-ccc7-4cba-81f7-144d5c3e3d88",
   "metadata": {
    "id": "be981c3e-ccc7-4cba-81f7-144d5c3e3d88",
    "outputId": "f6d72439-e756-4c7b-fa08-cb06a5507d00",
    "ExecuteTime": {
     "end_time": "2024-05-22T13:11:50.827040Z",
     "start_time": "2024-05-22T13:11:50.787037Z"
    }
   },
   "source": [
    "# Beam Search\n",
    "result = tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       num_beams=2, \n",
    "                       no_repeat_ngram_size=2,\n",
    "                       early_stopping=True\n",
    "                      )[0])\n",
    "print(result)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Beam Search\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241m.\u001B[39mdecode(model\u001B[38;5;241m.\u001B[39mgenerate(inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m      3\u001B[0m                        max_length\u001B[38;5;241m=\u001B[39mresult_length, \n\u001B[1;32m      4\u001B[0m                        num_beams\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, \n\u001B[1;32m      5\u001B[0m                        no_repeat_ngram_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m      6\u001B[0m                        early_stopping\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m      7\u001B[0m                       )[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(result)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1e88d-5c47-4802-9fb1-49704155e83f",
   "metadata": {
    "id": "76b1e88d-5c47-4802-9fb1-49704155e83f",
    "outputId": "582e42ec-f4b9-4ad7-aa81-1485e7934b21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ketanji opened the file and stared at its contents, she wondered whether it was any relation to her, she had never before thought about writing a book and about a character named Nando.\n",
      "In the back of the book, she was surprised to\n"
     ]
    }
   ],
   "source": [
    "# Sampling Top-k + Top-p\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac70a2-a27e-4cbd-9f6c-df419784646a",
   "metadata": {
    "id": "e9ac70a2-a27e-4cbd-9f6c-df419784646a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "venv",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "colab": {
   "name": "bloomex_nb.ipynb",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
