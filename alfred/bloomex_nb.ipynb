{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed037877ed4beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eba4f35-ed22-4de8-8124-49f100a4a593",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T19:27:21.084301Z",
     "start_time": "2024-05-24T19:27:19.412666Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomForTokenClassification\n",
    "from transformers import BloomForTokenClassification\n",
    "from transformers import BloomTokenizerFast\n",
    "import torch\n",
    "\n",
    "#import importlib\n",
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e306dd3-50db-43f1-b7ff-67c84d318535",
   "metadata": {
    "id": "5e306dd3-50db-43f1-b7ff-67c84d318535"
   },
   "source": [
    "# Bloom for Causal Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8879880-3c68-450b-adb1-f7a597c2f375",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T13:13:08.532665Z",
     "start_time": "2024-05-22T13:12:04.277232Z"
    },
    "id": "d8879880-3c68-450b-adb1-f7a597c2f375"
   },
   "outputs": [],
   "source": [
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\") #, local_files_only=True)\n",
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\") #, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60eb2324-38f7-4a8c-afca-683c8fce0cd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T13:13:08.564077Z",
     "start_time": "2024-05-22T13:13:08.536475Z"
    },
    "id": "60eb2324-38f7-4a8c-afca-683c8fce0cd2"
   },
   "outputs": [],
   "source": [
    "#prompt = \"Ketanji opened the file and stared at its contents, she\"\n",
    "prompt = \"From Baltimore, MD we have that Baltimore is in the state of Maryland.\"\n",
    "result_length = 50\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed9f3fef-8dbb-47ec-99a3-61e6a6c947df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T13:13:08.573Z",
     "start_time": "2024-05-22T13:13:08.567345Z"
    },
    "id": "ed9f3fef-8dbb-47ec-99a3-61e6a6c947df"
   },
   "outputs": [],
   "source": [
    "#outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "#loss = outputs.loss\n",
    "#logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db68bc2-f556-4353-a40a-28db2f71cdaf",
   "metadata": {
    "id": "2db68bc2-f556-4353-a40a-28db2f71cdaf",
    "outputId": "47170a55-4efb-4904-8c4b-52a75eb1cc0e"
   },
   "outputs": [],
   "source": [
    "# Greedy Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                       max_length=result_length,\n",
    "                       no_repeat_ngram_size=2\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be981c3e-ccc7-4cba-81f7-144d5c3e3d88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T13:14:14.540611Z",
     "start_time": "2024-05-22T13:13:44.124644Z"
    },
    "id": "be981c3e-ccc7-4cba-81f7-144d5c3e3d88",
    "outputId": "f6d72439-e756-4c7b-fa08-cb06a5507d00"
   },
   "outputs": [],
   "source": [
    "# Beam Search\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       num_beams=2, \n",
    "                       no_repeat_ngram_size=2,\n",
    "                       early_stopping=True\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1e88d-5c47-4802-9fb1-49704155e83f",
   "metadata": {
    "id": "76b1e88d-5c47-4802-9fb1-49704155e83f",
    "outputId": "582e42ec-f4b9-4ad7-aa81-1485e7934b21"
   },
   "outputs": [],
   "source": [
    "# Sampling Top-k + Top-p\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c6da33-cd3b-4b10-9152-05e0e4c21bd9",
   "metadata": {
    "id": "e9ac70a2-a27e-4cbd-9f6c-df419784646a"
   },
   "source": [
    "# Alfred testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee45e07-3236-4aa3-b758-edf3cbf44a34",
   "metadata": {},
   "source": [
    "## APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "722f5544-4bbb-4baa-9f10-9af756f710cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_api.ipynb.\n",
    "\n",
    "# %% auto 0\n",
    "__all__ = ['BaseAPI', 'CalculatorAPI', 'WolframAPI', 'WeatherAPI', 'LocationAPI']\n",
    "\n",
    "import random\n",
    "# %% ../nbs/03_api.ipynb 4\n",
    "from abc import abstractclassmethod\n",
    "\n",
    "import wolframalpha\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# %% ../nbs/03_api.ipynb 6\n",
    "class BaseAPI:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str, # the name of the API call\n",
    "        prompt_template: PromptTemplate,\n",
    "        sampling_threshold: float = 0.2,\n",
    "        filtering_threshold: float = 0.2,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.prompt_template = prompt_template\n",
    "        self.sampling_threshold = sampling_threshold\n",
    "        self.filtering_threshold = filtering_threshold\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def execute(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, *args: str, **kargs: str) -> str:\n",
    "        output = self.execute(*args, **kargs)\n",
    "        return str(output)\n",
    "\n",
    "# %% ../nbs/03_api.ipynb 8\n",
    "class CalculatorAPI(BaseAPI):\n",
    "    def execute(self, input: str) -> str:\n",
    "        try:\n",
    "            return eval(input)\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "# %% ../nbs/03_api.ipynb 10\n",
    "class WolframAPI(BaseAPI):\n",
    "    def __init__(self, *args, api_key: str, **kargs):\n",
    "        super().__init__(*args, **kargs)\n",
    "        self.api_key = api_key\n",
    "        \n",
    "    def execute(self, input: str) -> str:\n",
    "        client = wolframalpha.Client(self.api_key)\n",
    "        res = client.query(input=input)\n",
    "        return next(res.results).text\n",
    "\n",
    "class WeatherAPI(BaseAPI):\n",
    "    def __init__(self, *args, **kargs):\n",
    "        super().__init__(*args, **kargs)\n",
    "        #self.api_key = api_key\n",
    "        #raise NotImplementedError\n",
    "\n",
    "\n",
    "    #def get_temperature(self, latitude: float, longitude: float) -> int:\n",
    "    def get_temperature(self, city: str) -> int:\n",
    "        #client = wolframalpha.Client(self.api_key)\n",
    "        #res = client.query(input=input)\n",
    "\n",
    "        # check if city is valid\n",
    "        def validCity(city: str):\n",
    "            df = pd.read_csv('../../uscities.csv')\n",
    "            return (df == city).any().any()\n",
    "            #return random.random() > 0.5\n",
    "\n",
    "\n",
    "        # returns the current temperature of the city\n",
    "        if validCity(city):\n",
    "            # TODO integrate with weather API\n",
    "            # weatherAPI.com, api key: ecfeef9758ff455084241925242105\n",
    "            return random.randint(40, 80)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid city name\")\n",
    "\n",
    "        # command = f\"curl -L $(curl -L https://api.weather.gov/points/{latitude},{longitude} | jq --raw-output .properties.forecast) | jq  --raw-output .properties.periods[0].temperature\"\n",
    "        # # print(command)\n",
    "        # ps = subprocess.Popen(command, shell=True, executable='/bin/bash', stdin=subprocess.DEVNULL,\n",
    "        #                       stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        # result = ps.stdout.read().decode(\"utf-8\")\n",
    "        # return int(result)\n",
    "\n",
    "    def execute(self, input: str) -> str:\n",
    "        #client = wolframalpha.Client(self.api_key)\n",
    "        #res = client.query(input=input)\n",
    "        # now sure how to handle generic string text, when the weatherAPI requires specific inputs\n",
    "        try:\n",
    "            return str(self.get_temperature(input))\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "class LocationAPI(BaseAPI):\n",
    "    def __init__(self, *args, cities_csv: str, **kargs):\n",
    "        super().__init__(*args, **kargs)\n",
    "        self.df = pd.read_csv(cities_csv)\n",
    "        #self.api_key = api_key\n",
    "        #raise NotImplementedError\n",
    "\n",
    "\n",
    "    # #def get_temperature(self, latitude: float, longitude: float) -> int:\n",
    "    # def get_state(self, city: str) -> int:\n",
    "    #     #client = wolframalpha.Client(self.api_key)\n",
    "    #     #res = client.query(input=input)\n",
    "\n",
    "        # check if city is valid\n",
    "    #def validCity(city: str):\n",
    "\n",
    "        # # returns the current temperature of the city\n",
    "        # if validCity(city):\n",
    "        #     # weatherAPI.com, api key: ecfeef9758ff455084241925242105\n",
    "        #     return random.randint(40, 80)\n",
    "        # else:\n",
    "        #     raise ValueError(\"Invalid city name\")\n",
    "\n",
    "        # command = f\"curl -L $(curl -L https://api.weather.gov/points/{latitude},{longitude} | jq --raw-output .properties.forecast) | jq  --raw-output .properties.periods[0].temperature\"\n",
    "        # # print(command)\n",
    "        # ps = subprocess.Popen(command, shell=True, executable='/bin/bash', stdin=subprocess.DEVNULL,\n",
    "        #                       stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        # result = ps.stdout.read().decode(\"utf-8\")\n",
    "        # return int(result)\n",
    "\n",
    "    def execute(self, input: str) -> str:\n",
    "        #client = wolframalpha.Client(self.api_key)\n",
    "        #res = client.query(input=input)\n",
    "        # now sure how to handle generic string text, when the weatherAPI requires specific inputs\n",
    "        # return (df == city).any().any()\n",
    "        # return random.random() > 0.5\n",
    "        result = self.df.loc[:, ['city', 'state_name']][self.df['city'] == \"Baltimore\"]\n",
    "        # return result.size\n",
    "        if result.size > 0:\n",
    "            return result.iloc[0]['state_name']\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SearchAPI(BaseAPI):\n",
    "    def __init__(self, *args, api_key: str, **kargs):\n",
    "        super().__init__(*args, **kargs)\n",
    "        #self.api_key = api_key\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93d3e4f-0fb8-4aca-9501-9d54ea5c9b0f",
   "metadata": {},
   "source": [
    "## DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70dfee21-4188-41a0-a8fd-b5ce44acc01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del DataGenerator\n",
    "\n",
    "# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_data_generator.ipynb.\n",
    "\n",
    "# %% auto 0\n",
    "__all__ = ['AugmentedCandidate', 'DataGenerator']\n",
    "\n",
    "# %% ../nbs/04_data_generator.ipynb 4\n",
    "import re\n",
    "from typing import List, Callable, Tuple, Union, TypedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtyping import TensorType\n",
    "from einops import rearrange\n",
    "\n",
    "#from .api import BaseAPI\n",
    "\n",
    "# %% ../nbs/04_data_generator.ipynb 5\n",
    "class AugmentedCandidate(TypedDict):\n",
    "    api_start_positions: int\n",
    "\n",
    "# %% ../nbs/04_data_generator.ipynb 6\n",
    "class DataGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict,\n",
    "        model: Callable, tokenizer: Callable,\n",
    "        apis: List[BaseAPI],\n",
    "        device: str = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ):\n",
    "        super().__init__()\n",
    "        start_character = config[\"data_generator\"][\"api_start_character\"]\n",
    "        end_character = config[\"data_generator\"][\"api_end_character\"]\n",
    "        output_character = config[\"data_generator\"][\"api_output_character\"]\n",
    "        \n",
    "        # add a space, because when the model generate a token, it's also include a \"space\"\n",
    "        self.api_start_token_id = tokenizer(f' {start_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_end_token_id = tokenizer(end_character, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_output_token_id = tokenizer(f'{output_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        \n",
    "        self.top_k_sampling = config[\"data_generator\"][\"top_k_sampling\"]\n",
    "        self.sampling_threshold = config[\"data_generator\"][\"sampling_threshold\"]\n",
    "        self.filtering_threshold = config[\"data_generator\"][\"filtering_threshold\"]\n",
    "        \n",
    "        self.apis = apis\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "        # TODO: handle for cases that the sentence contains \".\\n\\n\"\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.eos_token_id = tokenizer(\".\\n\\n\")[\"input_ids\"][0]\n",
    "    \n",
    "    def sample_api_position(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"seq_len\"], # the ids of the prompt\n",
    "    ) -> Tuple[\n",
    "        TensorType[\"n_positions\"], # The positions of api call\n",
    "        TensorType[\"seq_len\"] # The generated text\n",
    "    ]:\n",
    "        \"\"\"Sampling API positions.\"\"\"\n",
    "        # TODO: add support batch\n",
    "        # the ids of the prompt and generated_ids\n",
    "        prompt_and_generated_ids = prompt_ids.to(self.device)\n",
    "        # only the ids of the generated_ids\n",
    "        generated_ids = torch.tensor([]).to(self.device)\n",
    "        i = torch.tensor([0]).to(self.device)\n",
    "        \n",
    "        api_pos_probs = torch.tensor([]).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():    \n",
    "            while True:\n",
    "                logits = self.model(\n",
    "input_ids=prompt_and_generated_ids.unsqueeze(0)).logits\n",
    "\n",
    "                # Alfred returns the last row of second dimension, and all the columns of third dimension\n",
    "                last_logit = logits[0, -1, :]\n",
    "                probs = torch.softmax(last_logit, dim=-1)\n",
    "                api_start_prob = probs[self.api_start_token_id].to(self.device)\n",
    "                \n",
    "                # Alfred: returns tensor stack of index having api_start_prob greater than threshold\n",
    "                if api_start_prob > self.sampling_threshold:\n",
    "                    print(\"adding sampled api_position\")\n",
    "                    api_pos_probs = torch.cat([\n",
    "                        api_pos_probs,\n",
    "                        torch.tensor([api_start_prob, i]).unsqueeze(0).to(self.device)\n",
    "                    ], dim=0)     \n",
    "                \n",
    "                # sampling a token\n",
    "                # next_token = torch.multinomial(probs, num_samples=1)\n",
    "                next_token = torch.argmax(probs, dim=-1)\n",
    "                next_token = next_token.unsqueeze(0)\n",
    "                \n",
    "                prompt_and_generated_ids = torch.cat([prompt_and_generated_ids, next_token], dim=0)\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=0).to(self.device)\n",
    "                \n",
    "                if next_token == self.eos_token_id:\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"adding sampling iterations {int(i)}\")\n",
    "        \n",
    "        if api_pos_probs.numel() == 0:\n",
    "            api_positions = torch.tensor([]).to(self.device)\n",
    "        else:\n",
    "            _, indices = torch.sort(api_pos_probs[:, 0], descending=True)\n",
    "            top_k_sampling = self.top_k_sampling\n",
    "            api_positions = api_pos_probs[indices[:top_k_sampling], 1]\n",
    "        return api_positions.long(), generated_ids.long()\n",
    "\n",
    "    def obtain_api_response(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"seq_len\"],\n",
    "        positions: TensorType[\"n_positions\"],\n",
    "        generated_ids: TensorType[\"seq_len\"]\n",
    "    ) -> TensorType[\"n_positions\", \"seq_len\"]:\n",
    "        \n",
    "        MAX_PAD = 50\n",
    "        \n",
    "        # the ids before the start of an api call\n",
    "        pre_api_ids = torch.tensor([]).to(self.device)\n",
    "\n",
    "        for position in positions:\n",
    "            #print(generated_ids[:position])\n",
    "            #print(self.api_start_token_id)\n",
    "            text_ids = torch.cat([generated_ids[:position], self.api_start_token_id.to(self.device)], dim=0)\n",
    "            padded_text_ids = F.pad(text_ids, pad=(MAX_PAD - text_ids.shape[-1], 0), value=self.pad_token_id).to(self.device)\n",
    "\n",
    "            #print(pre_api_ids)\n",
    "            #print(padded_text_ids)\n",
    "            pre_api_ids = torch.cat([\n",
    "                pre_api_ids,\n",
    "                rearrange(padded_text_ids, \"... -> 1 ...\")\n",
    "            ])\n",
    "        \n",
    "        PROMPT_LENGTH = len(prompt_ids)\n",
    "        \n",
    "        # TODO: optimzie this\n",
    "        prompt_and_pre_api_ids = torch.tensor([]).to(self.device)\n",
    "        for x in pre_api_ids:\n",
    "            prompt_and_pre_api_ids = torch.cat([\n",
    "                prompt_and_pre_api_ids,\n",
    "                torch.cat([prompt_ids, x]).unsqueeze(0)\n",
    "            ], dim=0)\n",
    "                     \n",
    "        with torch.no_grad():\n",
    "            candidate_ids = self.model.generate(\n",
    "                input_ids=prompt_and_pre_api_ids.long(),\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                max_new_tokens=50,\n",
    "            )\n",
    "        \n",
    "        # filter out the prompt template\n",
    "        # only keep the generated ids\n",
    "        candidate_ids = candidate_ids[:, PROMPT_LENGTH:]\n",
    "        \n",
    "        return candidate_ids\n",
    "    \n",
    "    def _generate_conditioning_prompts(\n",
    "        self,\n",
    "        api: BaseAPI,\n",
    "        candidate_ids: TensorType[\"n_candidates\", \"seq_len\"],\n",
    "    ):\n",
    "        conditioning_api_ids = torch.tensor([])\n",
    "\n",
    "        API_NAME = api.name\n",
    "        MAX_PAD = 100\n",
    "        \n",
    "        def extract_api_request_content(text: str, api_name: str) -> str:\n",
    "            \"\"\"Extract the content of an API request from a given text.\"\"\"\n",
    "            start_tag = f\"{api_name}(\"\n",
    "            end_tag = \")\"\n",
    "            start_idx = text.find(start_tag)\n",
    "            if start_idx == -1:\n",
    "                return None\n",
    "            start_idx += len(start_tag)\n",
    "            end_idx = text.find(end_tag, start_idx)\n",
    "            if end_idx == -1:\n",
    "                return None\n",
    "            return text[start_idx:end_idx]\n",
    "        \n",
    "        def extract_api_syntax(text: str, api_name: str) -> str:\n",
    "            \"\"\"Extract the API Syntax from a given text.\"\"\"\n",
    "            pattern = r\"\\[{}\\(.*?\\)\\]\".format(api_name)\n",
    "            matches = re.findall(pattern, text)\n",
    "            return matches\n",
    "\n",
    "        for text_ids in candidate_ids:\n",
    "            # the ids of the prediction\n",
    "            text = self.tokenizer.decode(text_ids, skip_special_tokens=True)\n",
    "            \n",
    "            api_request_content = extract_api_request_content(text, api_name=API_NAME)\n",
    "            api_response = api(api_request_content)\n",
    "            api_response_ids = self.tokenizer(api_response, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            # Format: \"-> [api_response]\"\n",
    "            api_response_with_arrow_ids = torch.cat([self.api_output_token_id, api_response_ids], dim=0)\n",
    "            \n",
    "            api_syntax = extract_api_syntax(text, api_name=API_NAME)\n",
    "            api_syntax_ids = self.tokenizer(api_syntax, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            api_syntax_with_response_ids = torch.cat([api_syntax_ids[:-1], api_response_with_arrow_ids, api_syntax_ids[-1:]])\n",
    "            api_syntax_without_response_ids = torch.cat([api_syntax_ids[:-1], self.api_output_token_id, api_syntax_ids[-1:]])\n",
    "                              \n",
    "            padded_api_without_response = rearrange(\n",
    "                F.pad(api_syntax_without_response_ids, pad=((MAX_PAD - api_syntax_without_response_ids.shape[-1]), 0), value=self.pad_token_id),\n",
    "                \"... -> 1 ...\"\n",
    "            )\n",
    "            padded_api_with_response = rearrange(\n",
    "                F.pad(api_syntax_with_response_ids, pad=((MAX_PAD - api_syntax_with_response_ids.shape[-1]), 0), value=self.pad_token_id),\n",
    "                \"... -> 1 ...\"\n",
    "            )\n",
    "        \n",
    "            padded_api_call = torch.cat([\n",
    "                padded_api_without_response,\n",
    "                padded_api_with_response\n",
    "            ], dim=0)\n",
    "            padded_api_call = rearrange(padded_api_call, \"... -> 1 ...\")\n",
    "            \n",
    "            conditioning_api_ids = torch.cat([conditioning_api_ids, padded_api_call], dim=0).long()\n",
    "                    \n",
    "        return conditioning_api_ids\n",
    "\n",
    "    def _filter_candidate_by_threshold(\n",
    "        self,\n",
    "        losses,\n",
    "        candidates: TensorType[\"seq_len\"]\n",
    "    ):\n",
    "        filtered_augmented_text_ids = torch.tensor([]).to(self.device)\n",
    "        for i, position in enumerate(losses):\n",
    "            negative_loss = min(losses[position][0], losses[position][1])\n",
    "            positive_loss = losses[position][2]\n",
    "            \n",
    "            if negative_loss - positive_loss >= self.filtering_threshold:\n",
    "                # filtered_augmented_text_ids.append(candidates[i])\n",
    "                filtered_augmented_text_ids = torch.cat([\n",
    "                    filtered_augmented_text_ids,\n",
    "                    candidates[i].unsqueeze(0)\n",
    "                ], dim=0)\n",
    "        \n",
    "        return filtered_augmented_text_ids.long()\n",
    "\n",
    "    def filter_api( \n",
    "        self,\n",
    "        api: BaseAPI,\n",
    "        text_ids: TensorType[\"seq_len\"],\n",
    "        api_start_idxs: TensorType[\"n_positions\"],\n",
    "        candidate_ids: TensorType[\"n_positions\", \"seq_len\"]\n",
    "    ):\n",
    "        conditioning_api_ids = self._generate_conditioning_prompts(api, candidate_ids)\n",
    "                \n",
    "        SPACE_TOKEN = self.tokenizer(\". \", return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        API_LENGTH = 100\n",
    "        augmented_text_ids = {\"api_start_positions\": {}}\n",
    "        \n",
    "        def _compute_weight(t: int) -> Union[int, float]:\n",
    "            \"\"\"Compute the weight in the loss function.\"\"\"\n",
    "            return max(0, 1-0.2*t)\n",
    "        \n",
    "        for idx, api_ids in zip(api_start_idxs, conditioning_api_ids):\n",
    "            idx = idx.item()\n",
    "            seq_len = len(text_ids)\n",
    "            augmented_text_ids[\"api_start_positions\"][idx] = {\n",
    "                \"seq_positions\": {}\n",
    "            }\n",
    "\n",
    "            j = idx\n",
    "            while j <= seq_len - 1:\n",
    "                # if the model predic\n",
    "                if j == 1:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                \n",
    "                # in the formua, from x_1 to x_j (include x_j)\n",
    "                # => generate_ids[:j]\n",
    "                conditioning_text_ids = text_ids[:j]\n",
    "                api_and_text_ids = torch.stack([\n",
    "                    F.pad(conditioning_text_ids, pad=(API_LENGTH + len(SPACE_TOKEN), 0), value=self.pad_token_id), # [text_ids]\n",
    "                    torch.cat([api_ids[0], SPACE_TOKEN, conditioning_text_ids], dim=0), # [api->, text_ids]\n",
    "                    torch.cat([api_ids[1], SPACE_TOKEN, conditioning_text_ids], dim=0), # [api->result, text_ids]\n",
    "                ], dim=0)\n",
    "                                \n",
    "                # the next token after x_j\n",
    "                next_token_ids = text_ids[j]\n",
    "                augmented_text_ids[\"api_start_positions\"][idx][\"seq_positions\"][j] = {\n",
    "                    \"prompt_ids\": api_and_text_ids,\n",
    "                    \"unnormalized_weight\": _compute_weight(t=j-idx),\n",
    "                    \"losses\": [],\n",
    "                    \"target_ids\": torch.tensor([next_token_ids, next_token_ids, next_token_ids])\n",
    "                }\n",
    "                j += 1\n",
    "        \n",
    "        def _normalize_weights(augmented_text_ids):\n",
    "            \"\"\"Normalize the weight of each position in a sequence.\"\"\"\n",
    "            for api_start_position in augmented_text_ids[\"api_start_positions\"].values():\n",
    "                total_weight = sum([seq_position[\"unnormalized_weight\"] for seq_position in api_start_position[\"seq_positions\"].values()])\n",
    "                for seq_position in api_start_position[\"seq_positions\"].values():\n",
    "                    seq_position[\"normalized_weight\"] = seq_position[\"unnormalized_weight\"] / total_weight\n",
    "            \n",
    "            return augmented_text_ids\n",
    "        \n",
    "        augmented_text_ids = _normalize_weights(augmented_text_ids)\n",
    "                \n",
    "        def extract_conditioning_ids_and_target_ids(augmented_text_ids):\n",
    "            conditioning_text_ids = torch.tensor([])\n",
    "            target_ids = torch.tensor([])\n",
    "            \n",
    "            for _, api_start_position_dict in augmented_text_ids[\"api_start_positions\"].items():\n",
    "                for _, seq_position_dict in api_start_position_dict[\"seq_positions\"].items():\n",
    "                    target_ids = torch.concat([target_ids, seq_position_dict[\"target_ids\"]], dim=0)\n",
    "                    for prompt_id in seq_position_dict[\"prompt_ids\"]:\n",
    "                        conditioning_text_ids = torch.cat([\n",
    "                            conditioning_text_ids,\n",
    "                            F.pad(prompt_id.long(), pad=(50-prompt_id.shape[-1], 0), value=self.pad_token_id).unsqueeze(0)\n",
    "                        ], dim=0)\n",
    "        \n",
    "            return conditioning_text_ids.long(), target_ids.long()\n",
    "\n",
    "        conditioning_text_ids, target_ids = extract_conditioning_ids_and_target_ids(augmented_text_ids)\n",
    "            \n",
    "        output = self.model(input_ids=conditioning_text_ids.long().to(self.device))\n",
    "        logits = output.logits[:, -1, :]\n",
    "                    \n",
    "        def extract_target_logprob_from_logits(logits, target_ids):\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            target_log_probs = log_probs[range(target_ids.shape[-1]), target_ids]\n",
    "            return target_log_probs\n",
    "\n",
    "        log_probs = extract_target_logprob_from_logits(logits, target_ids)\n",
    "            \n",
    "        for _, api_start_position_dict in augmented_text_ids[\"api_start_positions\"].items():\n",
    "            for _, seq_position_dict in api_start_position_dict[\"seq_positions\"].items():\n",
    "                seq_position_dict[\"losses\"] = log_probs[:3].squeeze(0)\n",
    "                log_probs = log_probs[3:]\n",
    "        \n",
    "        def _calculate_weighted_loss(augmented_text_ids):\n",
    "            for position in augmented_text_ids[\"api_start_positions\"]:        \n",
    "                seq_positions = augmented_text_ids[\"api_start_positions\"][position][\"seq_positions\"]\n",
    "                for i in seq_positions:\n",
    "                    losses = seq_positions[i][\"losses\"]\n",
    "                    weights = seq_positions[i][\"normalized_weight\"]\n",
    "                    seq_positions[i][\"weighted_losses\"] = -losses * weights\n",
    "            \n",
    "            return augmented_text_ids\n",
    "        \n",
    "        augmented_text_ids = _calculate_weighted_loss(augmented_text_ids)\n",
    "        \n",
    "        def _calculate_loss(augmented_text_ids):\n",
    "            data = {}\n",
    "            for position in augmented_text_ids[\"api_start_positions\"]:        \n",
    "                seq_positions = augmented_text_ids[\"api_start_positions\"][position][\"seq_positions\"]\n",
    "                losses = [0, 0, 0]            \n",
    "                for i in seq_positions:\n",
    "                    losses[0] += seq_positions[i][\"weighted_losses\"][0] # loss for [text]\n",
    "                    losses[1] += seq_positions[i][\"weighted_losses\"][1] # loss for [api->, text]\n",
    "                    losses[2] += seq_positions[i][\"weighted_losses\"][2] # loss for [api-result, text]\n",
    "                data[position] = losses\n",
    "                \n",
    "            return data\n",
    "        \n",
    "        losses = _calculate_loss(augmented_text_ids)\n",
    "        print(f\"losses are {losses}\")\n",
    "        filtered_candidate_ids = self._filter_candidate_by_threshold(losses, candidate_ids)\n",
    "        return filtered_candidate_ids\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        text: str,\n",
    "    ) -> TensorType[\"n_apis\", \"n_candidates\", \"seq_len\"]:\n",
    "        filtered_apis = torch.tensor([]).to(self.device)\n",
    "        \n",
    "        for api in self.apis:\n",
    "            # TODO: add support batch\n",
    "            prompt = api.prompt_template.format(input=text)\n",
    "            prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            prompt_ids = prompt_ids.to(self.device)\n",
    "        \n",
    "            # sampling positions\n",
    "            print(\"started generating sample positions\")\n",
    "            api_start_idxs, generated_ids = self.sample_api_position(prompt_ids)\n",
    "    \n",
    "            # obtaining api responses\n",
    "            #print(prompt_ids)\n",
    "            #print(api_start_idxs)\n",
    "            #print(generated_ids)\n",
    "            print(\"started generating candidate_ids\")\n",
    "            candidate_ids = self.obtain_api_response(prompt_ids, api_start_idxs, generated_ids)\n",
    "\n",
    "            # filtering\n",
    "            print(\"started generating text_ids\")\n",
    "            text_ids = self.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            \n",
    "            # return prompt_ids, api_start_idxs, generated_ids, candidate_ids, text_ids\n",
    "            print(\"started filtering candidate_ids\")\n",
    "            filtered_candidate_ids = self.filter_api(api, text_ids, api_start_idxs, candidate_ids)\n",
    "                    \n",
    "            filtered_apis = torch.cat([filtered_apis, filtered_candidate_ids.unsqueeze(0)], dim=0)\n",
    "        \n",
    "        return filtered_apis.long()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4839d06-e6c8-4b1a-b842-98713d8733aa",
   "metadata": {},
   "source": [
    "## Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a29606d-75eb-4858-927c-7c1f568df65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del DataGenerator\n",
    "del LocationAPI\n",
    "#importlib.reload(toolformer)\n",
    "#from toolformer.data_generator import DataGenerator\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2175f5f9-dff6-499a-8786-273305dd54aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#del DataGenerator\n",
    "#importlib.reload(toolformer)\n",
    "#from toolformer.data_generator import DataGenerator\n",
    "#from toolformer.api import LocationAPI\n",
    "from toolformer.prompt import qa_prompt\n",
    "from toolformer.utils import yaml2dict\n",
    "\n",
    "config = yaml2dict('../configs/default.yaml')\n",
    "location_api = LocationAPI(\n",
    "    \"Location\", qa_prompt, cities_csv='uscities.csv',\n",
    "    sampling_threshold=0.2, filtering_threshold=0.2\n",
    ")\n",
    "\n",
    "#device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "\n",
    "#text = \"What is the temperature in Baltimore, MD?\"\n",
    "text = \"From Baltimore, MD we have that Baltimore is in the state of Maryland.\"\n",
    "#text = \"39.2896246543727, -76.58026446823449\"  # Patterson Park, Baltimore, MD\n",
    "#text = \"From this, we have 10 - 5 minutes = 5 minutes.\"\n",
    "apis = [location_api]\n",
    "generator = DataGenerator(config, model, tokenizer, apis=apis)\n",
    "\n",
    "augmented_text_ids = generator.generate(text)\n",
    "\n",
    "print(tokenizer.decode(augmented_text_ids[0][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96065275-2c1f-4bcd-b568-a6ca943c8f8c",
   "metadata": {},
   "source": [
    "## Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb42b37b98334b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T19:25:45.469208Z",
     "start_time": "2024-05-24T19:25:43.066652Z"
    }
   },
   "outputs": [],
   "source": [
    "#del toolformer\n",
    "#%from toolformer.data_generator aimport DataGenerator\n",
    "#%autoreload 1\n",
    "#!pip install -e ../\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#importlib.reload(toolformer.data_generator)\n",
    "\n",
    "# use copy and paste code from above instead of module code\n",
    "#from toolformer.data_generator import DataGenerator\n",
    "#import toolformer.data_generator\n",
    "#from toolformer.api import CalculatorAPI\n",
    "from toolformer.prompt import calculator_prompt\n",
    "from toolformer.utils import yaml2dict\n",
    "\n",
    "#from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b23992be708548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T19:26:01.461790Z",
     "start_time": "2024-05-24T19:26:01.148469Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "config = yaml2dict('../configs/default.yaml')\n",
    "calculator_api = CalculatorAPI(\n",
    "    \"Calculator\", calculator_prompt,\n",
    "    sampling_threshold=0.2, filtering_threshold=0.2\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "\n",
    "text = \"From this, we have 10 - 5 minutes = 5 minutes.\"\n",
    "apis = [calculator_api]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e389d6452822c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started generating sample positions\n",
      "adding sampled api_position\n",
      "started generating candidate_ids\n",
      "started generating text_ids\n",
      "started filtering candidate_ids\n",
      "losses are {10: [tensor(2.7667, device='cuda:0', grad_fn=<AddBackward0>), tensor(2.2431, device='cuda:0', grad_fn=<AddBackward0>), tensor(2.1710, device='cuda:0', grad_fn=<AddBackward0>)]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#del DataGenerator\n",
    "#from toolformer.data_generator import DataGenerator\n",
    "#importlib.reload(toolformer.data_generator)\n",
    "generator = DataGenerator(config, model, tokenizer, apis=apis)\n",
    "\n",
    "augmented_text_ids = generator.generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "763175b2-e4e4-4d75-9e03-488ee352da41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T19:27:29.668414Z",
     "start_time": "2024-05-24T19:27:29.597960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From this, we have 10 - 5 minutes = [Calculator(10 - 5)].\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ids = augmented_text_ids[0][0]\n",
    "ids = torch.tensor([[[     3,      3,      3,      3,      3,      3,      3,      3,\n",
    "               3,      3,      3,      3,      3,      3,      3,      3,\n",
    "               3,      3,      3,      3,      3,      3,      3,      3,\n",
    "               3,      3,      3,      3,      3,      3,      3,      3,\n",
    "               3,      3,      3,      3,      3,      3,      3,  12620,\n",
    "            1119,     15,   1701,   1542,   1581,    647,    973,  17405,\n",
    "             564,   1111, 120009,   2623,     11,   1416,    647,    973,\n",
    "              12,     64,   6149]]], device='cuda:0')\n",
    "ids = ids[0][0]\n",
    "result = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da235b079502214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "bloomex_nb.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "venv",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
