{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Overview\n",
    "\n",
    "**Note: All the images are from the Credits section or the internet. And this one was not reviewed completely because no enough computing resources, and bf16 type is not supported by current env.**\n",
    "\n",
    "Pre-trained LLMs can only perform next-token prediction, making them unable to answer questions. This is why these base models are then fine-tuned on pairs of instructions and anwers to act as helpful assistants. However, this process can still be flawed: fine-tuned LLMs can be biased, toxic, harmful, etc. This is where Reinforcement Learning from RLHF(Reinforcement Learning from Human Feedback) comes into play.\n",
    "\n",
    "RLHF provides different answers to the LLM, which are ranked according to a desired behavior(helpfulness, toxicity, etc). The model learns to output the best answer among these candidates, hence mimicking the behavior we want to instill. Often seen as a way to censor models, this process has recently become popular for improving performance, as shown in [neural-chat-7b-v3-1](https://huggingface.co/Intel/neural-chat-7b-v3-1).\n",
    "\n",
    "We are going to fine-tune OpenHermes-2.5 using a RLHF-like technique: **Direct Preference Optimization(DPO)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers==4.36.2\n",
    "!pip install datasets==2.15.0\n",
    "!pip install peft==0.7.1\n",
    "!pip install bitsandbytes==0.41.3\n",
    "!pip install trl==0.7.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/host/cs234_final/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'user_secrets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#from kaggle_secrets import UserSecretsClient\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#user_secrets = UserSecretsClient()\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m login(token\u001b[38;5;241m=\u001b[39m\u001b[43muser_secrets\u001b[49m\u001b[38;5;241m.\u001b[39mget_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGINGFACE_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# os.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# os.environ[\"WANDB_PROJECT\"] = \"Fine-tune-models\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# os.environ[\"WANDB_NOTES\"] = \"Fine tune model distilbert base uncased\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# os.environ[\"WANDB_NAME\"] = \"ft-openhermes-25-mistral-7b-irca-dpo-pairs\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'user_secrets' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "#from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "#user_secrets = UserSecretsClient()\n",
    "\n",
    "login(token=user_secrets.get_secret(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "# os.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "# os.environ[\"WANDB_PROJECT\"] = \"Fine-tune-models\"\n",
    "# os.environ[\"WANDB_NOTES\"] = \"Fine tune model distilbert base uncased\"\n",
    "# os.environ[\"WANDB_NAME\"] = \"ft-openhermes-25-mistral-7b-irca-dpo-pairs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing datasets\n",
    "\n",
    "We are prefer the **Preference datasets**. Typically consist of a collection of answers that are ranked by humans. This ranking is seential, as the RLHF process fine-tunes LLMs to output the preferred answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 196/196 [00:00<00:00, 1.71MB/s]\n",
      "Downloading data: 100%|██████████| 36.3M/36.3M [00:01<00:00, 30.0MB/s]\n",
      "Generating train split: 100%|██████████| 12859/12859 [00:00<00:00, 190229.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_ds=load_dataset(\"Intel/orca_dpo_pairs\")[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the dataset is straightforward: for each row, there is one chosen(preferred) answer, and one rejected answer. The goal of RLHF is to guide the model to output the preferred answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preference datasets\n",
    "\n",
    "They are notoriously costly and difficult to make, as they require collecting manual feedback from humans. This feedback is also subjective and can easily be biased toward confident(but wrong) answers or contradict itself (different annotators have different values). Over time, several solutions have been proposed to tackle these issues, such as replacing human feedback with AI feedback [RLAIF](https://arxiv.org/abs/2212.08073).\n",
    "\n",
    "These datasets also tend to be a lot smaller than fine-tuninf datasets. To illustrate this, the excellent [neural-chat-7b-v3-1](https://huggingface.co/Intel/neural-chat-7b-v3-1) uses 518k samples for fine-tuning [Open-Orca/SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca) but only 12.9k samples for RLHF [Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs). In this case, the authors generated answers with GPT-4/3.5 to create the preferred answers, and with [Llama 2 13b chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) to create the rejected responses. It's a smart way to bypass human feedback and only rely on models with different levels of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Preference Optimization\n",
    "\n",
    "While the concept of RLHF has been used in robotics for a long time, it was popularized for LLMs in [Fine-tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593.pdf). In this paper, the authors present a framework where a reward model is trained to approximate human feedback. This reward model is then used to optimize the fine-tuned model's policy using the [Proximal Policy Optimization algorithm](https://arxiv.org/abs/1707.06347).\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/707/466/912/510/214/original/5f3fa6b5a0b186a3.webp\" width=\"60%\" heigh=\"60%\" alt=\"proximal policy optimization\"></div>\n",
    "\n",
    "\n",
    "The Core concept of PPO revolves around making smaller, incremental updates to the policy, as larger updates can lead to instability or suboptimal solutions. From experience, this technique is unfortunately still unstable(loss diverges), difficult to reproduce(numerous hyperparameters, sensitive to random seeds), and computationally expensive.\n",
    "\n",
    "This is where Direct Preference Optimization(DPO) comes into play. DPO simplifies control by treating the task as a classification problem. Concretely, it uses two models: the **trained model(or policy model)** and a copy of it called the **reference model**. During training, the goal is to make sure the trained model outputs higher probabilities for preferred answers than the reference model. Conversely, we also want it output lower probabilities for rejected answers. It means we're penalizing the LLM for bad answers and rewarding it for good ones.\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/707/528/109/748/161/original/7c1ef620f48dd4a6.webp\" width=\"60%\" heigh=\"60%\" alt=\"direct preference optimization\"></div>\n",
    "\n",
    "By using the LLM itself as a reward model and employing binary cross-entropy objectives, DPO efficiently aligns the model's outputs with human preferences without the need for extensive sampling, reward model fitting, or intricate hyperparameters adjustments. It results in a more stable, more efficient, and computationally less demanding process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting the data\n",
    "\n",
    "Here we are going to fine-tune [OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B) which is a Mistral-7b model that was only supervised fine-tuned. To this end, we will use the [Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs) dataset to align our model and improve this performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/peft\n",
      "  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-_4dcbg8u\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-_4dcbg8u\n",
      "  Resolved https://github.com/huggingface/peft to commit 03798a9143c90d796a0bee8f43863668d084381f\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.2.dev0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.2.dev0) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.11.2.dev0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.11.2.dev0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.2.dev0) (2.2.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.11.2.dev0) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.11.2.dev0) (4.65.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.2.dev0) (0.30.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.11.2.dev0) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.2.dev0) (0.23.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.2.dev0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.2.dev0) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.2.dev0) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.2.dev0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.2.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.2.dev0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.2.dev0) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.11.2.dev0) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.11.2.dev0) (0.15.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.11.2.dev0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.2.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.2.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.2.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.2.dev0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.11.2.dev0) (1.3.0)\n",
      "Building wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.11.2.dev0-py3-none-any.whl size=255725 sha256=0a720b3850cbff30d1e38d6d5e343a615792d2d1f6824b4a5af9e37d0db39c2f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-m87s43r5/wheels/4c/16/67/1002a2d4daa822eff130e6d85b90051b75d2ce0d26b9448e4a\n",
      "Successfully built peft\n",
      "Installing collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.11.1\n",
      "    Uninstalling peft-0.11.1:\n",
      "      Successfully uninstalled peft-0.11.1\n",
      "Successfully installed peft-0.11.2.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mpeft                      0.11.2.dev0\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U peft\n",
    "!pip install git+https://github.com/huggingface/peft\n",
    "!pip list | grep peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'peft'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, PeftModel, get_peft_model\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DPOTrainer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peft'"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from trl import DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T04:52:19.189157Z",
     "iopub.status.busy": "2024-02-23T04:52:19.187501Z",
     "iopub.status.idle": "2024-02-23T04:52:19.19602Z",
     "shell.execute_reply": "2024-02-23T04:52:19.194074Z",
     "shell.execute_reply.started": "2024-02-23T04:52:19.189006Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name=\"teknium/OpenHermes-2.5-Mistral-7B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenHermes-2.5-Mistral-7B uses a specific chat template, called [ChatML](https://huggingface.co/docs/transformers/chat_templating). Here is an example of conversation formatted with this template:\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n",
    "<|im_start|>user\n",
    "How are you?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "I'm doing great!<|im_end|>\n",
    "```\n",
    "\n",
    "Each of sentence which includes a role, like \"system, \"user\" or \"assistant\", and it appends special tokens at the beginning and the end of the sentence(<|im_start|> and <|im_end|>). These two are used to separate different sentences. Moreover, DPOTrainer also requires a specific format with three columns: prompt, chosen and rejected.\n",
    "\n",
    "We will simply concatenate the system and question columns to the prompt column. We will also map the chatgpt column to \"chosen\" and llama2-13b-chat to \"rejected\". To format the dataset in a reliable way, we will use the tokenizer's `apply_chat_template()` function, which already use ChatML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T04:52:19.198326Z",
     "iopub.status.busy": "2024-02-23T04:52:19.197423Z",
     "iopub.status.idle": "2024-02-23T04:52:23.294915Z",
     "shell.execute_reply": "2024-02-23T04:52:23.293951Z",
     "shell.execute_reply.started": "2024-02-23T04:52:19.198291Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def chatml_format(example):\n",
    "    # format system\n",
    "    if len(example['system'])>0:\n",
    "        message ={\"role\":\"system\",\"content\":example['system']}\n",
    "        system=tokenizer.apply_chat_template([message], tokenize=False)\n",
    "    else:\n",
    "        system=\"\"\n",
    "    \n",
    "    #format instruction\n",
    "    message={\"role\":\"user\",\"content\":example['question']}\n",
    "    prompt=tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # format chosen answer\n",
    "    chosen =example['chosen']+\"<im_end>\\n\"\n",
    "    \n",
    "    # format rejected answer\n",
    "    rejected = example['rejected']+\"<im_end>\\n\"\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": system+prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }\n",
    "\n",
    "# we have load datasets in above section\n",
    "\n",
    "# save columns\n",
    "original_columns=train_ds.column_names\n",
    "\n",
    "# tokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.padding_side=\"left\"\n",
    "\n",
    "#format dataset\n",
    "train_dataset=train_ds.map(\n",
    "    function=chatml_format,\n",
    "    remove_columns=original_columns\n",
    ")\n",
    "\n",
    "# checking only one example\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model with DPO\n",
    "\n",
    "We define the LoRA configurations to train the model. We set the rank value to be equal to the `lora_alpha`, which is unusual(2*`r` as a rule of thumb). We also target all the linear modules with adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T04:52:23.296445Z",
     "iopub.status.busy": "2024-02-23T04:52:23.296158Z",
     "iopub.status.idle": "2024-02-23T04:52:23.304542Z",
     "shell.execute_reply": "2024-02-23T04:52:23.303581Z",
     "shell.execute_reply.started": "2024-02-23T04:52:23.29642Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "\n",
    "import torch\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config=LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['k_proj','gate_proj','v_proj','up_proj','q_proj','o_proj','down_proj']\n",
    ")\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "#     llm_int8_enable_fp32_cpu_offload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize the model\n",
    "\n",
    "We're now ready to load the model we want to fine-tune with DPO. In this case, two models are required: the model to fine-tune as well as the reference model. This is mostly for the sake of readability, as the `DPOTrainer` object automatically creates a ference model if none is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T04:52:23.306979Z",
     "iopub.status.busy": "2024-02-23T04:52:23.3065Z",
     "iopub.status.idle": "2024-02-23T04:54:23.805055Z",
     "shell.execute_reply": "2024-02-23T04:54:23.803922Z",
     "shell.execute_reply.started": "2024-02-23T04:52:23.30694Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T04:54:23.807115Z",
     "iopub.status.busy": "2024-02-23T04:54:23.806703Z",
     "iopub.status.idle": "2024-02-23T04:54:24.541892Z",
     "shell.execute_reply": "2024-02-23T04:54:24.540748Z",
     "shell.execute_reply.started": "2024-02-23T04:54:23.807078Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.use_cache=False\n",
    "model=get_peft_model(model, peft_config)\n",
    "model.get_memory_footprint()\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T04:54:24.543556Z",
     "iopub.status.busy": "2024-02-23T04:54:24.543248Z",
     "iopub.status.idle": "2024-02-23T04:54:39.86197Z",
     "shell.execute_reply": "2024-02-23T04:54:39.860979Z",
     "shell.execute_reply.started": "2024-02-23T04:54:24.543529Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reference model\n",
    "ref_model=AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "ref_model=get_peft_model(ref_model, peft_config)\n",
    "ref_model.get_memory_footprint()\n",
    "ref_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step consists of providing all the hyperparameters to `TrainingArguments` and `DPOTrainer`:\n",
    "* Among them, the `beta` parameter is unique to DPO since it controls the divergence from the initial policy(0.1 is a typical value for it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-23T04:55:14.957558Z",
     "iopub.status.busy": "2024-02-23T04:55:14.957163Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "\n",
    "training_args=TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    # key word argumnts to be passed to the gradient_checkingpointing_enable method\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=5,\n",
    "    remove_unused_columns=False,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # the total number of training steps to perform\n",
    "    max_steps=100,\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=100,\n",
    "    output_dir=os.getenv(\"WANDB_NAME\"),\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    bf16=False, # Doesn;t support in Kaggle environment\n",
    "    fp16=True,\n",
    "    # Number of steps used for a linear warmup from 0 to learning_rate\n",
    "    warmup_steps=50,\n",
    "    run_name=os.getenv(\"WANDB_NAME\"),\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "dpo_trainer=DPOTrainer(\n",
    "    model,\n",
    "    ref_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    beta=0.1,\n",
    "    max_prompt_length=1024,\n",
    "    max_length=1536,\n",
    ")\n",
    "\n",
    "# Kaggle env does not have enough Memory to doing training, it will restarted and cause error.\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-23T04:54:39.872836Z",
     "iopub.status.idle": "2024-02-23T04:54:39.873289Z",
     "shell.execute_reply": "2024-02-23T04:54:39.873069Z",
     "shell.execute_reply.started": "2024-02-23T04:54:39.873048Z"
    }
   },
   "outputs": [],
   "source": [
    "dpo_trainer.model.save_pretrained(\"final_checkpoint\")\n",
    "tokenizer.save_pretrained(\"final_checkpoint\")\n",
    "\n",
    "# Flush memory\n",
    "del dpo_trainer, model, ref_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload model in FP16(instead of NF4)\n",
    "base_model=AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    return_dict=True,\n",
    "    torch_type=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# merge base model with the adapter\n",
    "model=PeftModel.from_pretrained(base_model, \"final_checkpoint\")\n",
    "model=model.merge_and_unload()\n",
    "\n",
    "# save model and tokenizer\n",
    "model.save_pretrained(os.getenv(\"WANDB_NAME\"))\n",
    "tokenizer.save_pretrained(os.getenv(\"WANDB_NAME\"))\n",
    "\n",
    "\n",
    "model.push_to_hub(os.getenv(\"WANDB_NAME\"))\n",
    "tokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the model performs\n",
    "\n",
    "Excepting run the model locally, we can also leverage the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) to evaluate it. As the process is quite resource-intensive, we can also directly submit it for evaluation on the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-23T04:54:39.874427Z",
     "iopub.status.idle": "2024-02-23T04:54:39.874846Z",
     "shell.execute_reply": "2024-02-23T04:54:39.874659Z",
     "shell.execute_reply.started": "2024-02-23T04:54:39.874637Z"
    }
   },
   "outputs": [],
   "source": [
    "# format prompt\n",
    "message=[\n",
    "    {\"role\":\"system\", \"content\":\"The weather is important.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Does Melborune raining today?\"}\n",
    "]\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(os.getenv(\"WANDB_NAME\"))\n",
    "prompt=tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "pipe=transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=os.getenv(\"WANDB_NAME\"),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# generate text\n",
    "sequences=pipe(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    max_length=200,\n",
    ")\n",
    "\n",
    "sequences[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "* https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
