{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be6543b-98c0-4f71-ade5-82e0450d9842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T00:08:05.668365Z",
     "start_time": "2024-06-05T00:08:00.821412Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import csv\n",
    "\n",
    "from dvc.repo import Repo\n",
    "\n",
    "import yaml\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8827c2a-82bc-47e3-941d-9cbef2eacc08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T00:08:05.668365Z",
     "start_time": "2024-06-05T00:08:00.821412Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b379bf6f4937b517",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash --out TOP_LEVEL\n",
    "printf \"$(git rev-parse --show-toplevel)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cc76d2cc310b37d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# set working directory to root of git repo\n",
    "config = yaml.safe_load(Path(TOP_LEVEL + '/configs/default.yaml').read_text())\n",
    "match config['model']['torch_dtype']:\n",
    "    case 'float16':\n",
    "        torch_dtype = torch.float16\n",
    "    case 'float32':\n",
    "        torch_dtype = torch.float32\n",
    "    case 'float64':\n",
    "        torch_dtype = torch.float64\n",
    "    case 'bfloat16':\n",
    "        torch_dtype = torch.bfloat16\n",
    "    case 'auto':\n",
    "        torch_dtype = \"auto\"\n",
    "    case _:\n",
    "        raise ValueError('torch_dtype is invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff70a2b-3550-4683-9c36-2ecf3988e264",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load model and dataset - dataset needs to be in a specific format\n",
    "model = AutoModelForCausalLM.from_pretrained(config[\"model\"][\"path\"],torch_dtype=torch_dtype).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"tokenizer\"][\"path\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c64bf36d-51e4-4c93-8fbf-4665bd2ff220",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN_EPOCHS = 20\n",
    "OUTPUT_DIR = TOP_LEVEL + f\"/alfred/output/{config['model']['path']},torch_dtype={torch_dtype}/epoch={NUM_TRAIN_EPOCHS}\"\n",
    "#os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6bc3dc3-4870-4319-a7f6-3e625dc64dc7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 737/737 [00:00<00:00, 4.41MB/s]\n",
      "Downloading data: 100%|██████████| 20/20 [01:56<00:00,  5.81s/files]\n",
      "Generating train split: 7435908 examples [00:34, 214998.45 examples/s]\n",
      "Map: 100%|██████████| 7435908/7435908 [01:00<00:00, 123196.71 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'chosen', 'rejected'],\n",
      "    num_rows: 7435908\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#from collections import Dict\n",
    "\n",
    "def return_prompt_and_responses(samples): # -> Dict[str, str, str]:\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            \"Question: \" + question + \"\\n\\nAnswer: \"\n",
    "            for question in samples[\"question\"]\n",
    "        ],\n",
    "        \"chosen\": samples[\"response_j\"],   # rated better than k\n",
    "        \"rejected\": samples[\"response_k\"], # rated worse than j\n",
    "    }\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"lvwerra/stack-exchange-paired\",\n",
    "    split=\"train\",\n",
    "    data_dir=\"data/rl\"\n",
    ")\n",
    "original_columns = dataset.column_names\n",
    "\n",
    "train_dataset = dataset.map(\n",
    "    return_prompt_and_responses,\n",
    "    batched=True,\n",
    "    remove_columns=original_columns\n",
    ")\n",
    "#train_dataset = train_dataset.select(range(100))\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "382a75b0-b8a3-4cd0-b518-c0677a11934a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# get dataset\n",
    "#train_dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "with open(TOP_LEVEL+'/generated_data/Weather.csv', mode='r') as f:\n",
    "    data_reader = csv.DictReader(f)\n",
    "    # with open('coors_new.csv', mode='w') as outfile:\n",
    "    #     writer = csv.writer(outfile)\n",
    "    #     mydict = {rows[0]:rows[1] for rows in reader}\n",
    "    dpo_dataset_dict = {}\n",
    "    dpo_dataset_dict[\"prompt\"] = []\n",
    "    dpo_dataset_dict[\"chosen\"] = []\n",
    "    dpo_dataset_dict[\"rejected\"] = []\n",
    "    for row in data_reader:\n",
    "        dpo_dataset_dict[\"prompt\"].append(row[\"input\"])\n",
    "        dpo_dataset_dict[\"chosen\"].append(row[\"text\"])\n",
    "        dpo_dataset_dict[\"rejected\"].append(row[\"candidate\"])\n",
    "\n",
    "# copy the dataset used to the output directory\n",
    "!cp {TOP_LEVEL+'/generated_data/Weather.csv'} {OUTPUT_DIR+'/Weather.csv'}\n",
    "\n",
    "train_dataset = Dataset.from_dict(dpo_dataset_dict)\n",
    "#print(train_dataset[\"prompt\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b438e770-84bc-4c0e-a194-93c16596f27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/host/cs234_final/venv/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:363: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/mnt/host/cs234_final/venv/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:376: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/mnt/host/cs234_final/venv/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:411: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map:  10%|█         | 775293/7435908 [58:44<13:51:38, 133.48 examples/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Map:  33%|███▎      | 2467229/7435908 [3:18:19<6:44:25, 204.76 examples/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Map:  56%|█████▌    | 4161173/7435908 [5:23:20<5:07:27, 177.52 examples/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Map:  58%|█████▊    | 4317542/7435908 [5:34:55<3:16:28, 264.53 examples/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Map:  78%|███████▊  | 5829834/7435908 [7:28:17<1:36:53, 276.26 examples/s]"
     ]
    }
   ],
   "source": [
    "# load trainer\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    beta=0.1,\n",
    "    # does not automatically save model output\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS\n",
    ")\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# train\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765892d-095c-4bb9-a44b-0b8618cce1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained(OUTPUT_DIR)\n",
    "print(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea2aa0-6813-4ab1-bfa0-02abd09f5a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "# with open(f\"{OUTPUT_DIR}/results.json\", \"w\") as f:\n",
    "#     json.dump(results.metrics, f)\n",
    "print(OUTPUT_DIR)\n",
    "model.save_pretrained(OUTPUT_DIR+'/final-model-save')\n",
    "trainer.save_model(OUTPUT_DIR+'/final-trainer-save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2c94a-3c7b-4251-a68d-452d2b113719",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T23:59:26.741360Z",
     "start_time": "2024-06-04T23:59:26.128101Z"
    }
   },
   "outputs": [],
   "source": [
    "# add log to dvc\n",
    "repo = Repo(\".\")\n",
    "#OUTPUT_PATH=\"/mnt/host/cs234_final/alfred/output/bigscience/bloom-560m,torch_dtype=float16/epoch=1000\"\n",
    "repo.add(OUTPUT_DIR)\n",
    "print('starting to push to remote')\n",
    "repo.push()\n",
    "!git add {TOP_LEVEL + '/alfred/output.dvc'}\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "# load the fine-tuned model\n",
    "print(torch_dtype)\n",
    "model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR + '/final-trainer-save', torch_dtype=torch_dtype).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR + '/final-trainer-save') #config[\"tokenizer\"][\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7d082bf-1fd6-4faf-bda4-31122e40b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([224619,   1728,    368,  43163,    861,   1306,  15984,    336,  23857,\n",
      "           632,    368,  60614,  29853,     34])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Echo all the statements that are provided.\\nWhy is the sky blue?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ids = torch.tensor(tokenizer(\"Echo all the statements that are provided.\\n\")[\"input_ids\"])\n",
    "query_ids = torch.tensor(tokenizer(\"Why is the sky blue?\")[\"input_ids\"])\n",
    "prompt_and_query_ids = torch.cat([prompt_ids, query_ids], dim=0)\n",
    "print(prompt_and_query_ids)\n",
    "tokenizer.decode(prompt_and_query_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b45117d-20a1-4f42-b395-31cc92a3fdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([224619,   1728,    368,  43163,    861,   1306,  15984,    336,  23857,\n",
      "           632,    368,  60614,  29853,     34,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Echo all the statements that are provided.\\nWhy is the sky blue?<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_ids = model.generate(\n",
    "                  torch.Tensor(prompt_and_query_ids).unsqueeze(0).to(\"cuda\"),\n",
    "    num_beams=1, max_new_tokens=100,              \n",
    "    repetition_penalty=1.2 #,temperature = 0\n",
    "            )\n",
    "print(response_ids[0])\n",
    "tokenizer.batch_decode(response_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3314dc50-b9a5-4a6e-8c59-19c6a3d1537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(input_ids=prompt_and_query_ids.unsqueeze(0).to(\"cuda\")).logits\n",
    "last_logit = logits[0, -1, :]\n",
    "probs = torch.softmax(last_logit, dim=-1)\n",
    "\n",
    "next_token = torch.argmax(probs, dim=-1)\n",
    "next_token = next_token.unsqueeze(0)\n",
    "tokenizer.decode(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "188abe78-ae1c-49d3-9391-378e5eec6d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a doctoral student after graduation, but I have been interested in the topic for a few years. As always it is a bit difficult to know exactly how to study if i am interested, but I want to get some idea about what would be a good introduction.\n",
      "My university is in Canada and I am currently studying philosophy and is a big fan of science fiction and fantasy fiction. I am not interested in science fiction and fantasy anymore, but I'm still interested in the idea of a big city like in\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "\n",
    "def generate(text):\n",
    "    cleaned = []\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)\n",
    "    generated_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    for i in generated_output:\n",
    "        print(i)\n",
    "\n",
    "generate(\"I am a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe7d20-d48a-4569-bdd4-667026ec6993",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=5,\n",
    "    num_beams=4,\n",
    "    num_return_sequences=4,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c347be-ad82-4def-ae99-a5fd24e8d324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
