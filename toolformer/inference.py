# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_data_generator.ipynb.

# %% auto 0
__all__ = ['TooledInference']

# %% ../nbs/04_data_generator.ipynb 4
import re
from typing import List, Callable, Tuple
from transformers import StoppingCriteria, StoppingCriteriaList

import torch
from torch import nn
import torch.nn.functional as F

from torchtyping import TensorType

from .api import BaseAPI
from .utils import ask_gpt

MAX_SEQ_LENGTH = 50

# %% ../nbs/04_data_generator.ipynb 6
class TooledInference(nn.Module):
    def __init__(
        self,
        config: dict,
        model: Callable, tokenizer: Callable,
        apis: List[BaseAPI],
        device: str = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    ):
        super().__init__()
        start_character = config["data_generator"]["api_start_character"]
        end_character = config["data_generator"]["api_end_character"]
        output_character = config["data_generator"]["api_output_character"]
        
        # add a space, because when the model generate a token, it's also include a "space"
        self.api_start_token_id = tokenizer(f' {start_character}', return_tensors="pt")["input_ids"][0]
        self.api_end_token_id = tokenizer(end_character, return_tensors="pt")["input_ids"][0]
        self.api_output_token_id = tokenizer(f'{output_character}', return_tensors="pt")["input_ids"][0]
        
        self.top_k_sampling = config["data_generator"]["top_k_sampling"]
        self.sampling_threshold = config["data_generator"]["sampling_threshold"]
        self.filtering_threshold = config["data_generator"]["filtering_threshold"]
        
        self.apis = apis
        self.model = model.to(device)
        self.tokenizer = tokenizer
        self.device = device
        
        # TODO: handle for cases that the sentence contains ".\n\n"
        self.pad_token_id = tokenizer.pad_token_id
        self.eos_token_id = tokenizer(".\n\n")["input_ids"][0]
    
    def generate_with_api_call(
        self,
        prompt_ids: TensorType["seq_len"], # the ids of the prompt
        bias_apis = False,
        estimated_length = None
    ) -> Tuple[
        TensorType["n_positions"], # The positions of api call
        TensorType["seq_len"] # The generated text
    ]:
        """Sampling API positions."""
        # TODO: add support batch
        # the ids of the prompt and generated_ids
        prompt_and_generated_ids = prompt_ids.to(self.device)
        # only the ids of the generated_ids
        generated_ids = torch.tensor([]).to(self.device)
        i = 0

        def remove_surrounding_quotes(s):
          if s.startswith('"') and s.endswith('"'):
              return s[1:-1]
          elif s.startswith("'") and s.endswith("'"):
              return s[1:-1]
          return s
      
        def extract_api_request_content(text: str, api_name: str) -> str:
            """Extract the content of an API request from a given text."""
            start_tag = f"{api_name}API("
            end_tag = ")"
            start_idx = text.find(start_tag)
            if start_idx == -1:
                return None
            start_idx += len(start_tag)
            end_idx = text.find(end_tag, start_idx)
            if end_idx == -1:
                return None
            return remove_surrounding_quotes(text[start_idx:end_idx])
        
        api_started = False
        max_gen_length =  MAX_SEQ_LENGTH
        if estimated_length:
            max_gen_length = min (MAX_SEQ_LENGTH, 1.1 * estimated_length)

        with torch.no_grad():    
            while i < max_gen_length:
                logits = self.model(input_ids=prompt_and_generated_ids.unsqueeze(0)).logits
                last_logit = logits[0, -1, :]
                probs = torch.softmax(last_logit, dim=-1)
                api_start_prob = probs[self.api_start_token_id].to(self.device)
                
                if bias_apis and api_start_prob > self.sampling_threshold:
                    next_token = torch.tensor([self.api_start_token_id]).to(self.device)
                else:
                    next_token = torch.argmax(probs, dim=-1)
                    next_token = next_token.unsqueeze(0)

                if i == max_gen_length - 1: 
                    # Prevent infinite generation
                    next_token = torch.tensor([self.eos_token_id]).to(self.device)
                
                prompt_and_generated_ids = torch.cat([prompt_and_generated_ids, next_token], dim=0)
                generated_ids = torch.cat([generated_ids, next_token], dim=0)
                
                if next_token == self.eos_token_id:
                    break
                if next_token == torch.tensor([self.api_start_token_id]).to(self.device):
                    api_idx = i
                    api_started = True
                if api_started and next_token == torch.tensor([self.api_end_token_id]).to(self.device):
                    text = self.tokenizer.decode(modified_generation_ids[api_idx:])
                    
                    api_request_content = None
                    for api in self.apis:
                        api_request_content = extract_api_request_content(text, api_name=api.name)
                    api_response = api(api_request_content)
                    api_response_ids = self.tokenizer(api_response, return_tensors="pt")["input_ids"][0].to(self.device)
                    modified_generation_ids = torch.cat(
                      [modified_generation_ids, torch.tensor([self.api_end_token_id]).to(self.device),
                      torch.tensor([self.api_output_token_id]).to(self.device),
                      api_response_ids], dim=0)
                    
                    api_started = False
                
                i += 1
                    
        return generated_ids.long()